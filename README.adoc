= Testbench for Zookeeper Backup Concept

This testbench allows to evaluate how Kafka behaves when a Zookeeper configuration is restored from backup.

This example also shows how Kafka Streams will handle this situation (exactly the same) and how how kStreams handles message headers. These are by default forwarded in the most simple case.

DISCLAIMER: This project is for demonstration purposes only. Using the concept in production is highly discouraged. Use at your own risk.

== Preconditions

Adapt the testbench to your version of Confluent Platform as required by modifying `.env`.

== Running Confluent Platform

Start the containers by running:
```
docker compose up -d
```

Stopping the containers without removing any volumes:
```
docker compose down
```

Stopping the containers with removal of all created volumes (be careful!):
```
docker compose down -v
```

Cleaning up (CAREFUL: THIS WILL DELETE ALL UNUSED VOLUMES):
```
docker volumes prune
```

== Initialize the environment

Create a topic with many partitions:

```shell
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --create --topic topic-1 --partitions 9 --replication-factor 3
```

Look at the result:

```shell
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --describe --topic topic-1
```

Produce to the topics:

```shell
docker compose exec broker1 bash -c '(for i in {1..9}; do echo "$i:Message $i"; done) | kafka-console-producer --bootstrap-server localhost:9092 --topic topic-1 --property "parse.key=true" --property "key.separator=:" --property "acks=all"'
```

Consume from the topic like this, including the headers of the messages:

```shell
docker compose exec broker1 kafka-console-consumer --bootstrap-server localhost:9092 \
    --from-beginning \
    --property print.key=true \
    --topic topic-1
```

== Test 1: Shutdown one broker, wait for leader re-assignment

```shell
docker compose down broker2
```

Look at the result:

```shell
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --describe --topic topic-1
```

Consume from the topic again (no message is lost as we have created 3 replicas):

```shell
docker compose exec broker1 kafka-console-consumer --bootstrap-server localhost:9092 \
    --from-beginning \
    --property print.key=true \
    --topic topic-1
```

Start broker again.

```shell
docker compose up broker2
```

Look at the result:

```shell
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --describe --topic topic-1
```

== Test 2: Create Zookeeper backup. Kill broker. Shutdown Zookeeper. Restore backup. Start Zookeeper again

Copy Zookeeper config without shutting down Zookeeper nodes:
```bash
mkdir backup
tar cjfv backup/test2.tar.bz2 data
```

Kill one of the brokers:
```bash
docker compose down broker2
```

Get the list of active brokers from Zookeeper. Note, that `broker2` (ID: 2) is missing as expected:

```bash
docker compose exec zookeeper1 zookeeper-shell localhost:21811 ls /brokers/ids
```


Shutdown zookeeper nodes:
```bash
docker compose down zookeeper1 zookeeper2 zookeeper3
```

Simulate loss of all Zookeeper node data (BE EXTRA CAREFUL WHEN RUNNING THIS COMMAND!):
```bash
rm -irf ./data/zk?/log/* ./data/zk?/data/*
rm ./backup/*.tar.bz2
```

Restore backup (where `broker2` was still active):
```bash
tar -xjvf backup/test2.tar.bz2
```

Start zookeeper nodes:
```bash
docker compose up zookeeper1 zookeeper2 zookeeper3 -d
```

Check the status of all zookeeper nodes:
```bash
docker compose exec broker1 bash -c 'echo ruok | nc zookeeper1 21811; echo ""; echo ruok | nc zookeeper2 21812; echo ""; echo ruok | nc zookeeper3 
21813; echo ""'
```

Result: Zookeeper seems to be able to handle this properly. Even though initially the ZK node for `broker2` exists as found in the outdated backup, Zookeeper recognizes that the broker is actually mssing.

Please note that the outcome of this particular experiment does not allow to conclude that this will always work!


== Test 3: Create Zookeeper backup. Create new topic. Shutdown Zookeeper. Restore backup. Start Zookeeper again. Check new topic

Copy Zookeeper config without shutting down Zookeeper nodes:
```bash
mkdir backup
tar cjfv backup/test3.tar.bz2 data
```

Create a new topic:
```bash
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --create --topic topic-2 --partitions 9 --replication-factor 3
```

Show all topics:
```bash
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --list
```

Shutdown zookeeper nodes:
```bash
docker compose down zookeeper1 zookeeper2 zookeeper3
```

Simulate loss of all Zookeeper node data (BE EXTRA CAREFUL WHEN RUNNING THIS COMMAND!):
```bash
rm -irf ./data/zk?/log/* ./data/zk?/data/*
```

Restore backup (where `broker2` was still active):
```bash
tar -xjvf backup/test3.tar.bz2
```

Start zookeeper nodes:
```bash
docker compose up zookeeper1 zookeeper2 zookeeper3 -d
```

Check the status of all zookeeper nodes:
```bash
docker compose exec broker1 bash -c 'echo ruok | nc zookeeper1 21811; echo ""; echo ruok | nc zookeeper2 21812; echo ""; echo ruok | nc zookeeper3 21813; echo ""'
```

```bash
docker compose exec zookeeper1 zookeeper-shell localhost:21811 ls /brokers/topics
```

Show all topics:
```bash
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --list
```

Result: The newly created topic is not known to Zookeeper anymore, but shown by Kafka. The state of Kafka persisted to Zookeeper does not reflect the state in Kafka anymore. It is unclear how Kafka will react on that (e.g. handle the existing data files for that topic). Complete loss of data might be the result.

Please note that the outcome of this particular experiment does not allow to conclude that this will always work!


== Useful commands

You can delete the auto-created topic like this:

```shell
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --delete --topic topic-1
docker compose exec broker1 kafka-topics --bootstrap-server localhost:9092 --delete --topic topic-2
```

Check status of all zookeeper nodes:

```shell
for PORT in 21811 21812 21813; do echo $PORT; (echo stats | nc localhost ${PORT}|grep -E "Mode|current"); done
```
